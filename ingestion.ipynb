{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dc772e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 documents.\n",
      "Created 17 chunks.\n",
      "\n",
      "Example Chunk:\n",
      "\n",
      "{'filename': 'Document 2 System Architecture Overview.txt', 'chunk_id': 0, 'text': 'Document 2: System Architecture Overview Title: HK Groups Trading Platform – Architecture Overview Version: 1.0 Authority Level: Medium 1. System Description HK Groups operates a cloud‑native microservices architecture supporting: Trade execution Market analytics Portfolio tracking User management 2. Core Components 2.1 Frontend Web application (React) Mobile applications (iOS / Android) 2.2 Backend Services Order Management Service Market Data Service Risk & Compliance Service User Account Serv'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# ----------------------------\n",
    "# 1️⃣ Load Documents\n",
    "# ----------------------------\n",
    "\n",
    "def load_documents(data_path: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Loads all .txt files from the data directory.\n",
    "    Returns list of {filename, text}\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            full_path = os.path.join(data_path, file)\n",
    "\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            documents.append({\n",
    "                \"filename\": file,\n",
    "                \"text\": text\n",
    "            })\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 2️⃣ Clean Text\n",
    "# ----------------------------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic cleaning:\n",
    "    - remove extra spaces\n",
    "    - normalize newlines\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 3️⃣ Chunking Function\n",
    "# ----------------------------\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        start += chunk_size - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 4️⃣ Chunk All Documents\n",
    "# ----------------------------\n",
    "\n",
    "def chunk_documents(documents: List[Dict], \n",
    "                    chunk_size: int = 500, \n",
    "                    overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Chunks each document and attaches metadata\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        cleaned = clean_text(doc[\"text\"])\n",
    "        chunks = chunk_text(cleaned, chunk_size, overlap)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"filename\": doc[\"filename\"],\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "    print(f\"Created {len(all_chunks)} chunks.\")\n",
    "    return all_chunks\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 5️⃣ Test Run\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = \"C:/Users/HEMIN/Desktop/AgenticAI/TrainingData\"\n",
    "\n",
    "    docs = load_documents(DATA_PATH)\n",
    "    chunks = chunk_documents(docs)\n",
    "\n",
    "    print(\"\\nExample Chunk:\\n\")\n",
    "    print(chunks[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
